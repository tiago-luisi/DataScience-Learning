{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiago-luisi/Mathematics-Statistics-DS-AI/blob/main/DSA_MATEST_Projeto1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbQRQ76YiF6s"
      },
      "source": [
        "<!-- Projeto Desenvolvido na Data Science Academy - www.datascienceacademy.com.br -->\n",
        "# <font color='white'>Data Science Academy</font>\n",
        "## <font color='white'>Matemática e Estatística Aplicada Para Data Science, Machine Learning e IA</font>\n",
        "## <font color='white'>Projeto 1</font>\n",
        "## <font color='white'>Vetores e Espaço Vetorial em Sistemas de Recomendação</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agaiN3AoiF6u"
      },
      "source": [
        "### Instalando e Carregando Pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEZ1TYDHiF6u",
        "outputId": "00bfa13d-14b0-437f-d450-b977b7cac64a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m1.3/1.6 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
        "# pip install -U nome_pacote\n",
        "\n",
        "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
        "# !pip install nome_pacote==versão_desejada\n",
        "\n",
        "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
        "\n",
        "# Instala o pacote watermark.\n",
        "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
        "!pip install -q -U watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "unEp9ITPKPtD"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import ast\n",
        "import nltk\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gpaw_aV7iF6v",
        "outputId": "d4655d40-cffc-48ab-8fe9-fc05f54f2803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Data Science Academy\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Versões dos pacotes usados neste jupyter notebook\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Data Science Academy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZkKLqwNiF6w"
      },
      "source": [
        "## Carregando e Compreendendo os Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "jD77QZ4piF6w",
        "outputId": "177ae991-4b9f-4f3d-85c2-94d69c0e8662"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dados/dataset_filmes.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d3663c88118d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dataset de filmes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_dsa_filmes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dados/dataset_filmes.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dados/dataset_filmes.csv'"
          ]
        }
      ],
      "source": [
        "# Dataset de filmes\n",
        "df_dsa_filmes = pd.read_csv(\"dados/dataset_filmes.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXy-yR04iF6w"
      },
      "outputs": [],
      "source": [
        "# Shape\n",
        "df_dsa_filmes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4FQgZVAiF6w"
      },
      "outputs": [],
      "source": [
        "# Amostra dos dados\n",
        "df_dsa_filmes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLAG2whiiF6w"
      },
      "outputs": [],
      "source": [
        "# Dataset de elenco dos filmes\n",
        "df_elenco = pd.read_csv(\"dados/dataset_elenco.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYNbMQrNiF6w"
      },
      "outputs": [],
      "source": [
        "# Shape\n",
        "df_elenco.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tX7CcX9iF6w"
      },
      "outputs": [],
      "source": [
        "# Amostra dos dados\n",
        "df_elenco.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPnAj1-DiF6w"
      },
      "source": [
        "## Organização dos Dados de Texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYRhb-8yiF6w"
      },
      "source": [
        "> Existe uma relação entre os 2 datasets. Vamos fazer o merge pela coluna comum entre eles, a coluna title."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuRiMPljiF6w"
      },
      "outputs": [],
      "source": [
        "# Merge dos dataframes\n",
        "df_dsa_filmes = df_dsa_filmes.merge(df_elenco, on = 'title')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5Ris8a2iF6x"
      },
      "outputs": [],
      "source": [
        "# Shape\n",
        "df_dsa_filmes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5Vxdt63Q9ZN"
      },
      "outputs": [],
      "source": [
        "# Amostra dos dados\n",
        "df_dsa_filmes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l60a99tsWLO-"
      },
      "outputs": [],
      "source": [
        "# Info\n",
        "df_dsa_filmes.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQyP-V6wiF6x"
      },
      "source": [
        "> Muitas colunas parecem não ser relevantes para nossa análise. Vamos manter apenas o que é relevante."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVVMDn2sVmkD"
      },
      "outputs": [],
      "source": [
        "# Filtrando o dataframe e mantendo somente as colunas consideradas mais relevantes\n",
        "df_dsa_filmes = df_dsa_filmes[['movie_id', 'title', 'overview', 'genres', 'keywords', 'cast', 'crew']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qnrBeHMiF6x"
      },
      "source": [
        "A decisão acima é sua, Cientista de Dados, com base no seu conhecimento sobre a área de negócio ou sua percepção sobre os dados. Tome a decisão, justique-a e siga em frente no processo de análise. Se mais tarde achou que não foi a melhor decisão, então volte, muda a decisão, justique novamente e siga em frente até chegar ao melhor resultado possível.\n",
        "\n",
        "Lembre-se: Estamos fazendo Ciência de Dados e ciência requer experimentação!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5NvQSnkWFrp"
      },
      "outputs": [],
      "source": [
        "df_dsa_filmes.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azCr7iWWiF6x"
      },
      "source": [
        "## Análise Exploratória e Limpeza dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ow-8DcGiF6x"
      },
      "outputs": [],
      "source": [
        "# Shape\n",
        "df_dsa_filmes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0Swq0g6WV-i"
      },
      "outputs": [],
      "source": [
        "# Checando valores ausentes\n",
        "df_dsa_filmes.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiHFwy1UXJbi"
      },
      "outputs": [],
      "source": [
        "# Removemos as 3 linhas com valores ausentes\n",
        "df_dsa_filmes.dropna(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEjYEDqfiF6x"
      },
      "outputs": [],
      "source": [
        "# Checando valores ausentes\n",
        "df_dsa_filmes.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbg2qmPGXoQC"
      },
      "outputs": [],
      "source": [
        "# Checando se há valores duplicados\n",
        "df_dsa_filmes.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQARiGgQiF6y"
      },
      "outputs": [],
      "source": [
        "# Shape\n",
        "df_dsa_filmes.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3US6_PQiF6y"
      },
      "source": [
        "## Processamento de Texto com Abstract Syntax Trees\n",
        "\n",
        "https://docs.python.org/3/library/ast.html\n",
        "\n",
        "Vamos começar ajustando a coluna de gênero de filme. Nesta coluna precisamos apenas dos nomes e não dos ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2ATS9m7iF6y"
      },
      "outputs": [],
      "source": [
        "df_dsa_filmes.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MwKKdIriF6y"
      },
      "source": [
        "https://docs.python.org/3/library/ast.html\n",
        "\n",
        "O módulo ast ajuda a descobrir programaticamente como é a gramática atual de um objeto, processando árvores da gramática de sintaxe abstrata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQM0-8Y45N7r"
      },
      "outputs": [],
      "source": [
        "# Vejamos um exemplo\n",
        "# Avaliamos a lista de dicionários de strings\n",
        "ast.literal_eval('[{\"id\": 28, \"name\": \"Action\"}, \\\n",
        "                   {\"id\": 12, \"name\": \"Adventure\"}, \\\n",
        "                   {\"id\": 14, \"name\": \"Fantasy\"}, \\\n",
        "                   {\"id\": 878, \"name\": \"Science Fiction\"}]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLhjszQl5BQU"
      },
      "outputs": [],
      "source": [
        "# Função de conversão\n",
        "# Este loop itera sobre os elementos de uma estrutura de dados.\n",
        "# Usamos ast.literal_eval(obj) para avaliar/converter a string obj em uma estrutura de dados Python.\n",
        "def converter(obj):\n",
        "    L = []\n",
        "    for i in ast.literal_eval(obj):\n",
        "        L.append(i['name'])\n",
        "    return L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkW3A58TiF6y"
      },
      "outputs": [],
      "source": [
        "# Vamos testar a função\n",
        "teste = converter('[{\"id\": 28, \"name\": \"Action\"}, \\\n",
        "                    {\"id\": 12, \"name\": \"Adventure\"}, \\\n",
        "                    {\"id\": 14, \"name\": \"Fantasy\"}, \\\n",
        "                    {\"id\": 878, \"name\": \"Science Fiction\"}]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUSsHABYiF6y"
      },
      "outputs": [],
      "source": [
        "print(teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mzgjAAr71uh"
      },
      "outputs": [],
      "source": [
        "# Aplicamos então a função ao nosso dataset, na coluna de gênero do filme\n",
        "df_dsa_filmes['genres'] = df_dsa_filmes['genres'].apply(converter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWXdTNhN8HsD"
      },
      "outputs": [],
      "source": [
        "df_dsa_filmes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUiaGjxU8RYE"
      },
      "outputs": [],
      "source": [
        "# Aplicamos então a função ao nosso dataset, na coluna de keywords do filme\n",
        "df_dsa_filmes['keywords'] = df_dsa_filmes['keywords'].apply(converter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUFi6CqI86Oi"
      },
      "outputs": [],
      "source": [
        "df_dsa_filmes.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRTIhkdEiF64"
      },
      "source": [
        "Vamos aplicar o ast à coluna de cast (elenco) mas extrair sobre 3 membros do elenco. Isso deve ser suficiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3FUaVTo87y6"
      },
      "outputs": [],
      "source": [
        "# Função de conversão\n",
        "def converter3(obj):\n",
        "\n",
        "    # Cria uma lista vazia chamada L para armazenar os nomes.\n",
        "    L = []\n",
        "\n",
        "    # Inicializa um contador para controlar o número de elementos adicionados à lista.\n",
        "    counter = 0\n",
        "\n",
        "    # Usa ast.literal_eval para avaliar a string 'obj' e iterar sobre os elementos resultantes.\n",
        "    for i in ast.literal_eval(obj):\n",
        "\n",
        "        # Verifica se o contador é diferente de 3.\n",
        "        if counter != 3:\n",
        "\n",
        "            # Adiciona o valor da chave 'name' do dicionário atual à lista L.\n",
        "            L.append(i['name'])\n",
        "\n",
        "            # Incrementa o contador.\n",
        "            counter += 1\n",
        "        else:\n",
        "\n",
        "            # Interrompe o loop se o contador chegar a 3.\n",
        "            break\n",
        "\n",
        "    # Retorna a lista L com até 3 nomes.\n",
        "    return L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlPdIzxyBlpo"
      },
      "outputs": [],
      "source": [
        "# Aplicamos então a função ao nosso dataset, na coluna de cast (elenco) do filme\n",
        "df_dsa_filmes['cast'] = df_dsa_filmes['cast'].apply(converter3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2tCaUZsBplY"
      },
      "outputs": [],
      "source": [
        "df_dsa_filmes.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uthv_t4piF65"
      },
      "source": [
        "A função abaixo vai extrair o nome do diretor do filme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7eAIaCsBr6Y"
      },
      "outputs": [],
      "source": [
        "# Função de conversão\n",
        "def fetch_director(obj):\n",
        "\n",
        "    # Cria uma lista vazia chamada L para armazenar o nome do diretor.\n",
        "    L = []\n",
        "\n",
        "    # Converte a string 'obj' em um objeto Python e itera sobre seus elementos.\n",
        "    for i in ast.literal_eval(obj):\n",
        "\n",
        "        # Verifica se o trabalho ('job') do elemento atual é 'Director'.\n",
        "        if i['job'] == 'Director':\n",
        "\n",
        "            # Adiciona o nome do diretor (valor da chave 'name') à lista L.\n",
        "            L.append(i['name'])\n",
        "\n",
        "            # Interrompe o loop após encontrar o primeiro diretor.\n",
        "            break\n",
        "\n",
        "    # Retorna a lista L, contendo o nome do diretor.\n",
        "    return L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckPsIYI4iF65"
      },
      "outputs": [],
      "source": [
        "# Aplicamos então a função ao nosso dataset, na coluna de crew (equipe) do filme\n",
        "df_dsa_filmes['crew'] = df_dsa_filmes['crew'].apply(fetch_director)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6BYz6tXE3C1"
      },
      "outputs": [],
      "source": [
        "df_dsa_filmes.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1T9fwv-iF65"
      },
      "source": [
        "## Limpeza dos Dados de Texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWa48xtGFLJn"
      },
      "outputs": [],
      "source": [
        "# Separamos a string da coluna overview por espaço em branco\n",
        "df_dsa_filmes['overview'] = df_dsa_filmes['overview'].apply(lambda x:x.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuoPB5LYG9OW"
      },
      "outputs": [],
      "source": [
        "df_dsa_filmes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6dDNNovIWve"
      },
      "outputs": [],
      "source": [
        "# Replace de espaço por vazio (remove o espaço)\n",
        "df_dsa_filmes['genres'] = df_dsa_filmes['genres'].apply(lambda x:[i.replace(\" \",\"\") for i in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgJDs0t-iF65"
      },
      "outputs": [],
      "source": [
        "# Replace de espaço por vazio (remove o espaço)\n",
        "df_dsa_filmes['keywords'] = df_dsa_filmes['keywords'].apply(lambda x:[i.replace(\" \",\"\") for i in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_7wXMwViF65"
      },
      "outputs": [],
      "source": [
        "# Replace de espaço por vazio (remove o espaço)\n",
        "df_dsa_filmes['cast'] = df_dsa_filmes['cast'].apply(lambda x:[i.replace(\" \",\"\") for i in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvPCxiqXiF65"
      },
      "outputs": [],
      "source": [
        "# Replace de espaço por vazio (remove o espaço)\n",
        "df_dsa_filmes['crew'] = df_dsa_filmes['crew'].apply(lambda x:[i.replace(\" \",\"\") for i in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORWkPKmIHeC1"
      },
      "outputs": [],
      "source": [
        "df_dsa_filmes.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Srx_yxMiF65"
      },
      "source": [
        "## Preparando os Dados Para Vetorização em Outro Espaço Vetorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpBquSbmI5ll"
      },
      "outputs": [],
      "source": [
        "# Criamos a coluna de tags, nesse caso um vetor de strings com os valores das colunas\n",
        "df_dsa_filmes['tags'] = df_dsa_filmes['overview'] + \\\n",
        "                        df_dsa_filmes['genres'] + \\\n",
        "                        df_dsa_filmes['keywords'] + \\\n",
        "                        df_dsa_filmes['cast'] + \\\n",
        "                        df_dsa_filmes['crew']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_pl3VZSJegF"
      },
      "outputs": [],
      "source": [
        "df_dsa_filmes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRGDkQ8OJflV"
      },
      "outputs": [],
      "source": [
        "# Dataset final\n",
        "df_dsa_filmes_novo = df_dsa_filmes[['movie_id', 'title', 'tags']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8bKjL8EJxHF"
      },
      "outputs": [],
      "source": [
        "df_dsa_filmes_novo.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n--PXSGSJyeF"
      },
      "outputs": [],
      "source": [
        "# Join das strings para simplificar o vetor\n",
        "df_dsa_filmes_novo['tags'] = df_dsa_filmes_novo['tags'].apply(lambda x:\" \".join(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL57AgKFKQW9"
      },
      "outputs": [],
      "source": [
        "# Coloca tudo em minúsculo para evitar diferença de palavras maiúsculo/minúsculo\n",
        "df_dsa_filmes_novo['tags'] = df_dsa_filmes_novo['tags'].apply(lambda x:x.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eplyotuJLFCF"
      },
      "outputs": [],
      "source": [
        "df_dsa_filmes_novo.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHlLYPlWiF66"
      },
      "source": [
        "## Parse e Vetorização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC2s7bl5iF66"
      },
      "outputs": [],
      "source": [
        "# Criamos o parser\n",
        "parser_ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnVNkZNEiF66"
      },
      "source": [
        "Stemming é o processo de redução de uma palavra ao seu radical que está ligado a sufixos e prefixos ou às raízes de palavras conhecidas como \"lemmas\". Stemming é importante na compreensão de linguagem natural e processamento de linguagem natural."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0H4tluViF66"
      },
      "outputs": [],
      "source": [
        "# Função de stemming\n",
        "def stem(text):\n",
        "\n",
        "    # Cria uma lista vazia chamada y para armazenar as palavras após o stemming.\n",
        "    y = []\n",
        "\n",
        "    # Divide a string de entrada 'text' em palavras e itera sobre elas.\n",
        "    for i in text.split():\n",
        "\n",
        "        # Realiza o stemming na palavra atual 'i' e adiciona o resultado à lista y.\n",
        "        y.append(parser_ps.stem(i))\n",
        "\n",
        "    # Retorna as palavras processadas como uma string, unindo-as com espaços.\n",
        "    return \" \".join(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t0Z7bHxiF66"
      },
      "outputs": [],
      "source": [
        "# Aplica a função à coluna de tags\n",
        "df_dsa_filmes_novo['tags'] = df_dsa_filmes_novo['tags'].apply(stem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CglROEyqiF66"
      },
      "outputs": [],
      "source": [
        "df_dsa_filmes_novo.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEwSc_QNLWYc"
      },
      "outputs": [],
      "source": [
        "# Cria o vetorizador com no máximo 5000 atributos\n",
        "cv = CountVectorizer(max_features = 5000, stop_words = 'english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk3LEf7eiF66"
      },
      "source": [
        "A vetorização no contexto de processamento de linguagem natural (PLN) é o processo de converter texto em uma representação numérica, geralmente na forma de vetores. Este processo é fundamental para que algoritmos de aprendizado de máquina possam trabalhar com dados textuais, uma vez que eles requerem entradas numéricas.\n",
        "\n",
        "A linha de código acima está criando uma instância da classe CountVectorizer do scikit-learn. Vamos detalhar o que acontece nesta linha:\n",
        "\n",
        "**CountVectorizer**: É uma técnica de vetorização que converte uma coleção de documentos de texto em uma matriz de contagens de tokens. Basicamente, ela conta quantas vezes cada palavra ocorre em cada documento.\n",
        "\n",
        "**max_features = 5000**: Este parâmetro limita o número de palavras mais frequentes a serem consideradas. Aqui, apenas as 5000 palavras mais frequentes em todo o conjunto de documentos serão mantidas.\n",
        "\n",
        "**stop_words = 'english'**: Especifica que as palavras de parada (stop words) em inglês devem ser removidas do texto. Palavras de parada são palavras que são filtradas antes ou após o processamento de texto, geralmente porque são muito comuns e carregam pouca informação significativa (por exemplo, \"the\", \"is\", \"and\" em inglês)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu5B41AHiF66"
      },
      "outputs": [],
      "source": [
        "# Cria os vetores para as tags\n",
        "vectors = cv.fit_transform(df_dsa_filmes_novo['tags']).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljGCy2bsiF66"
      },
      "outputs": [],
      "source": [
        "len(cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ0eA3uhiF66"
      },
      "outputs": [],
      "source": [
        "type(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ocs4NHGSiF66"
      },
      "outputs": [],
      "source": [
        "vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9PuU20ciF66"
      },
      "outputs": [],
      "source": [
        "# Para visualizar todas as colunas do array\n",
        "np.set_printoptions(threshold = np.inf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1my0aJ0MiF67"
      },
      "source": [
        "A matriz **vectors** gerada pela função fit_transform do CountVectorizer representa a frequência de cada palavra (após a aplicação de stemming) em cada documento (ou linha) da coluna de tags de seu dataframe. Aqui está o que significa cada elemento dessa matriz:\n",
        "\n",
        "0: Indica que a palavra correspondente a essa posição na matriz não aparece na linha específica do dataframe. Ou seja, a tag após o stemming não está presente naquele registro específico.\n",
        "\n",
        "1: Significa que a palavra aparece uma vez na linha do dataframe. Após o stemming, a tag foi identificada uma vez naquele registro.\n",
        "\n",
        "2 (ou mais): Indica que a palavra aparece múltiplas vezes. Um valor de 2 significa que, após o stemming, a tag específica foi encontrada duas vezes naquele registro, e assim por diante para valores maiores.\n",
        "\n",
        "Este comportamento é esperado porque o CountVectorizer constrói um vocabulário de todas as palavras únicas encontradas nos documentos fornecidos e, em seguida, conta quantas vezes cada palavra aparece em cada documento. O processo de stemming aplicado anteriormente por meio da função stem reduz as palavras a suas raízes, o que pode resultar na agregação de diferentes formas da mesma palavra como uma única entrada no vocabulário. Isso pode levar a um aumento na contagem (por exemplo, \"run\", \"running\", \"runs\" após stemming podem ser contados como a mesma palavra base, aumentando sua contagem em um documento específico)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GDkNEQJiF67"
      },
      "outputs": [],
      "source": [
        "vectors[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkE7NQViiF67"
      },
      "outputs": [],
      "source": [
        "vectors[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBKtHBV2iF67"
      },
      "source": [
        "Os vetores acima ajudam a explicar porque é difícil treinar LLMs (Large Language Models), o estado da arte hoje em IA. Imagine vetores como esses para idiomas completos!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0QatPdSiF67"
      },
      "source": [
        "## Cálculo de Distância dos Vetores\n",
        "\n",
        "A célula abaixo trabalha com os vetores no espaço de dimensão para calcular a distância matemática entre eles usando a distância de cosseno (cosine_similarity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CTRwo3SiF67"
      },
      "outputs": [],
      "source": [
        "# Calcula a similaridade entre os vetores calculando as distâncias entre eles\n",
        "similaridades = cosine_similarity(vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzUUcpbLiF67"
      },
      "source": [
        "## Construindo o Sistema de Recomendação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5cj-w02iF67"
      },
      "outputs": [],
      "source": [
        "# Função para o sistema de recomendação\n",
        "def sistema_recomendacao(movie):\n",
        "\n",
        "    # Obtém o índice do filme passado como argumento (o que o usuário assistiu)\n",
        "    index = df_dsa_filmes_novo[df_dsa_filmes_novo['title'] == movie].index[0]\n",
        "\n",
        "    # Verificamos então os filmes com vetores de menor distância para o filme passado como argumento\n",
        "    distances = sorted(list(enumerate(similaridades[index])), reverse = True, key = lambda x: x[1])\n",
        "\n",
        "    # E então consideramos os 5 filmes com menor distância, ou seja, maior similaridade\n",
        "    for i in distances[1:6]:\n",
        "        print(df_dsa_filmes_novo.iloc[i[0]].title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qdXIN-TiF67"
      },
      "source": [
        "## Aplicando o Sistema de Recomendação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NGlIj_aiF67"
      },
      "outputs": [],
      "source": [
        "# Quais as recomendações de filmes para quem assistiu ao filme: Avengers: Age of Ultron?\n",
        "sistema_recomendacao('Avengers: Age of Ultron')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJen9_4jiF67"
      },
      "outputs": [],
      "source": [
        "# Quais as recomendações de filmes para quem assistiu ao filme: Jurassic World?\n",
        "sistema_recomendacao('Jurassic World')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uspP5X9iF67"
      },
      "outputs": [],
      "source": [
        "# Quais as recomendações de filmes para quem assistiu ao filme: The Hobbit: The Battle of the Five Armies?\n",
        "sistema_recomendacao('The Hobbit: The Battle of the Five Armies')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EUuFWzciF67"
      },
      "outputs": [],
      "source": [
        "%reload_ext watermark\n",
        "%watermark -a \"Data Science Academy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yei1cM4yiF68"
      },
      "outputs": [],
      "source": [
        "#%watermark -v -m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGYRodSMiF68"
      },
      "outputs": [],
      "source": [
        "#%watermark --iversions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njqzL_L0iF68"
      },
      "source": [
        "# Fim"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "22c3c4ef68249025b66c35f1cd68752a0c2e38380ab29dc03f2e785c2f56e44e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}